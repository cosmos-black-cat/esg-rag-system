import json
import os
import re
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import torch
import numpy as np
from tqdm import tqdm

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI

sys.path.append(str(Path(__file__).parent))
from config import *

# ç°¡åŒ–å¾Œçš„é—œéµå­—é…ç½®
SIMPLIFIED_KEYWORDS = ["å†ç”Ÿå¡‘è† ", "å†ç”Ÿå¡‘æ–™", "å†ç”Ÿæ–™", "å†ç”Ÿpp"]

@dataclass
class FilteredResult:
    """å…©æ®µå¼ç¯©é¸çµæœ"""
    keyword: str
    value: str  # æ•¸å€¼æˆ–ç™¾åˆ†æ¯”
    paragraph: str  # å®Œæ•´æ®µè½
    page_number: str  # é ç¢¼
    confidence: float
    source_file: str  # ä¾†æºæª”æ¡ˆåç¨±

class TwoStageExtractor:
    """å…©æ®µå¼ESGè³‡æ–™ç¯©é¸æå–å™¨"""
    
    def __init__(self, vector_db_path: str = None):
        """åˆå§‹åŒ–æå–å™¨"""
        
        if vector_db_path is None:
            vector_db_path = VECTOR_DB_PATH
            
        self.vector_db_path = vector_db_path
        self.keywords = SIMPLIFIED_KEYWORDS
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        print("ğŸ“± è¼‰å…¥embeddingæ¨¡å‹...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        print("ğŸ“š è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {vector_db_path}")
        
        self.vector_db = FAISS.load_local(
            vector_db_path, 
            self.embedding_model,
            allow_dangerous_deserialization=True
        )
        
        # åˆå§‹åŒ–LLM
        print("ğŸ¤– åˆå§‹åŒ–Gemini LLM...")
        if not GOOGLE_API_KEY:
            raise ValueError("è«‹è¨­ç½® GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸")
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-pro",
            temperature=0.1,
            google_api_key=GOOGLE_API_KEY
        )

    def stage1_filter_documents(self) -> Dict[str, List[Document]]:
        """ç¬¬ä¸€éšæ®µï¼šç¯©é¸åŒ…å«é—œéµå­—çš„æ–‡æª”"""
        print("\nğŸ” ç¬¬ä¸€éšæ®µï¼šç¯©é¸åŒ…å«é—œéµå­—çš„ESGå ±å‘Šæ›¸...")
        
        # ç²å–æ‰€æœ‰æ–‡æª”
        all_docs = []
        for i in range(self.vector_db.index.ntotal):
            # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„å‘é‡è³‡æ–™åº«çµæ§‹ä¾†èª¿æ•´
            # ç›®å‰å…ˆç”¨æœå°‹çš„æ–¹å¼ä¾†ç²å–æ–‡æª”
            pass
        
        # ç‚ºæ¯å€‹é—œéµå­—æœå°‹ç›¸é—œæ–‡æª”
        filtered_docs = {}
        
        for keyword in self.keywords:
            print(f"  ğŸ” æœå°‹é—œéµå­—: {keyword}")
            
            # æœå°‹åŒ…å«è©²é—œéµå­—çš„æ–‡æª”
            search_results = self.vector_db.similarity_search_with_score(
                keyword, 
                k=50  # å¢åŠ æœå°‹æ•¸é‡ä»¥ç²å¾—æ›´å¤šç›¸é—œæ–‡æª”
            )
            
            # ç¯©é¸ç¢ºå¯¦åŒ…å«é—œéµå­—çš„æ–‡æª”
            relevant_docs = []
            for doc, score in search_results:
                if keyword in doc.page_content:
                    relevant_docs.append(doc)
            
            filtered_docs[keyword] = relevant_docs
            print(f"    âœ“ æ‰¾åˆ° {len(relevant_docs)} å€‹ç›¸é—œæ–‡æª”")
        
        return filtered_docs

    def extract_numbers_and_percentages(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–æ•¸å€¼å’Œç™¾åˆ†æ¯”"""
        patterns = [
            r'\d+\.?\d*\s*%',  # ç™¾åˆ†æ¯”ï¼š25.5%, 30%
            r'\d+\.?\d*\s*[kKgG]+',  # é‡é‡ï¼š100KG, 50kg
            r'\d+\.?\d*\s*å™¸',  # å™¸æ•¸ï¼š1000å™¸
            r'\d+\.?\d*\s*è¬å™¸',  # è¬å™¸ï¼š5.5è¬å™¸
            r'\d+\.?\d*\s*å…¬æ–¤',  # å…¬æ–¤
            r'\d+\.?\d*\s*å€',  # å€æ•¸ï¼š3å€
            r'\d+\.?\d*\s*æ¬¡',  # æ¬¡æ•¸ï¼š10æ¬¡
            r'\d+\.?\d*\s*å¹´',  # å¹´ä»½ç›¸é—œï¼š5å¹´
            r'\d{1,3}(?:,\d{3})*\.?\d*',  # å¤§æ•¸å­—ï¼š1,000, 50,000
        ]
        
        numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches)
        
        return list(set(numbers))  # å»é‡

    def stage2_extract_numeric_data(self, filtered_docs: Dict[str, List[Document]]) -> List[FilteredResult]:
        """ç¬¬äºŒéšæ®µï¼šæå–åŒ…å«æ•¸å€¼æˆ–ç™¾åˆ†æ¯”çš„æ®µè½"""
        print("\nğŸ“Š ç¬¬äºŒéšæ®µï¼šæå–åŒ…å«æ•¸å€¼æˆ–ç™¾åˆ†æ¯”çš„æ®µè½...")
        
        results = []
        
        for keyword, docs in filtered_docs.items():
            print(f"\n  ğŸ“‹ è™•ç†é—œéµå­—: {keyword} ({len(docs)} å€‹æ–‡æª”)")
            
            for doc in docs:
                # æª¢æŸ¥æ®µè½æ˜¯å¦åŒ…å«æ•¸å€¼æˆ–ç™¾åˆ†æ¯”
                numbers = self.extract_numbers_and_percentages(doc.page_content)
                
                if numbers:  # å¦‚æœæ‰¾åˆ°æ•¸å€¼æˆ–ç™¾åˆ†æ¯”
                    # ä½¿ç”¨LLMä¾†ç¢ºèªé€™äº›æ•¸å€¼æ˜¯å¦èˆ‡é—œéµå­—ç›¸é—œ
                    relevant_value = self.verify_numeric_relevance(keyword, doc.page_content, numbers)
                    
                    if relevant_value:
                        result = FilteredResult(
                            keyword=keyword,
                            value=relevant_value,
                            paragraph=doc.page_content.strip(),
                            page_number=f"ç¬¬{doc.metadata.get('page', 'unknown')}é ",
                            confidence=0.8,  # é€™è£¡å¯ä»¥æ ¹æ“šLLMçš„å›æ‡‰èª¿æ•´
                            source_file=doc.metadata.get('source', 'unknown')
                        )
                        results.append(result)
                        print(f"    âœ“ æ‰¾åˆ°ç›¸é—œæ•¸å€¼: {relevant_value}")
        
        return results

    def verify_numeric_relevance(self, keyword: str, text: str, numbers: List[str]) -> Optional[str]:
        """ä½¿ç”¨LLMé©—è­‰æ•¸å€¼æ˜¯å¦èˆ‡é—œéµå­—ç›¸é—œ"""
        
        prompt = f"""
è«‹åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­çš„æ•¸å€¼æ˜¯å¦èˆ‡é—œéµå­—"{keyword}"ç›´æ¥ç›¸é—œã€‚

æ–‡æœ¬å…§å®¹ï¼š
{text}

æ‰¾åˆ°çš„æ•¸å€¼ï¼š{', '.join(numbers)}

è«‹åˆ¤æ–·ï¼š
1. é€™äº›æ•¸å€¼ä¸­å“ªä¸€å€‹èˆ‡"{keyword}"æœ€ç›¸é—œï¼Ÿ
2. è©²æ•¸å€¼ä»£è¡¨ä»€éº¼å«ç¾©ï¼Ÿ

å¦‚æœæ²’æœ‰ç›´æ¥ç›¸é—œçš„æ•¸å€¼ï¼Œè«‹å›ç­”"ç„¡ç›¸é—œæ•¸å€¼"ã€‚
å¦‚æœæœ‰ç›¸é—œæ•¸å€¼ï¼Œè«‹åªå›ç­”æœ€ç›¸é—œçš„é‚£ä¸€å€‹æ•¸å€¼ï¼ˆåŒ…å«å–®ä½ï¼‰ã€‚

å›ç­”æ ¼å¼ï¼šæ•¸å€¼ æˆ– ç„¡ç›¸é—œæ•¸å€¼
"""
        
        try:
            response = self.llm.invoke(prompt)
            result = response.content.strip()
            
            # å¦‚æœå›ç­”ä¸æ˜¯"ç„¡ç›¸é—œæ•¸å€¼"ï¼Œå‰‡è¿”å›è©²æ•¸å€¼
            if result != "ç„¡ç›¸é—œæ•¸å€¼" and result not in ["æ²’æœ‰", "ç„¡", "æœªæ‰¾åˆ°"]:
                return result
            else:
                return None
        except Exception as e:
            print(f"    âš ï¸ LLMé©—è­‰å¤±æ•—: {e}")
            # å¦‚æœLLMå¤±æ•—ï¼Œè¿”å›ç¬¬ä¸€å€‹æ‰¾åˆ°çš„æ•¸å€¼
            return numbers[0] if numbers else None

    def run_two_stage_extraction(self) -> List[FilteredResult]:
        """åŸ·è¡Œå®Œæ•´çš„å…©æ®µå¼æå–"""
        print("ğŸš€ é–‹å§‹å…©æ®µå¼ESGè³‡æ–™æå–...")
        print("=" * 60)
        
        # ç¬¬ä¸€éšæ®µï¼šç¯©é¸æ–‡æª”
        filtered_docs = self.stage1_filter_documents()
        
        # çµ±è¨ˆç¬¬ä¸€éšæ®µçµæœ
        total_docs = sum(len(docs) for docs in filtered_docs.values())
        print(f"\nğŸ“ˆ ç¬¬ä¸€éšæ®µçµ±è¨ˆ:")
        print(f"  ç¯©é¸å‡ºæ–‡æª”ç¸½æ•¸: {total_docs}")
        for keyword, docs in filtered_docs.items():
            print(f"  {keyword}: {len(docs)} å€‹æ–‡æª”")
        
        # ç¬¬äºŒéšæ®µï¼šæå–æ•¸å€¼è³‡æ–™
        results = self.stage2_extract_numeric_data(filtered_docs)
        
        print(f"\nğŸ“Š ç¬¬äºŒéšæ®µçµ±è¨ˆ:")
        print(f"  æ‰¾åˆ°åŒ…å«æ•¸å€¼çš„æ®µè½: {len(results)} å€‹")
        
        return results

    def save_results_to_csv(self, results: List[FilteredResult]) -> str:
        """å°‡çµæœä¿å­˜ç‚ºCSVæª”æ¡ˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = RESULTS_PATH / f"two_stage_extraction_{timestamp}.csv"
        
        # è½‰æ›ç‚ºDataFrame
        data = []
        for result in results:
            data.append({
                'é—œéµå­—': result.keyword,
                'æ•¸å€¼æˆ–æ¯”ä¾‹': result.value,
                'æ®µè½å…§å®¹': result.paragraph,
                'é ç¢¼': result.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': result.confidence,
                'ä¾†æºæª”æ¡ˆ': result.source_file
            })
        
        df = pd.DataFrame(data)
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜è‡³: {csv_path}")
        return str(csv_path)

    def save_results_to_excel(self, results: List[FilteredResult]) -> str:
        """å°‡çµæœä¿å­˜ç‚ºExcelæª”æ¡ˆï¼ŒåŒ…å«çµ±è¨ˆè³‡è¨Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_path = RESULTS_PATH / f"two_stage_extraction_{timestamp}.xlsx"
        
        # è½‰æ›ç‚ºDataFrame
        data = []
        for result in results:
            data.append({
                'é—œéµå­—': result.keyword,
                'æ•¸å€¼æˆ–æ¯”ä¾‹': result.value,
                'æ®µè½å…§å®¹': result.paragraph,
                'é ç¢¼': result.page_number,
                'ä¿¡å¿ƒåˆ†æ•¸': result.confidence,
                'ä¾†æºæª”æ¡ˆ': result.source_file
            })
        
        df = pd.DataFrame(data)
        
        # å‰µå»ºçµ±è¨ˆè³‡è¨Š
        stats_data = []
        for keyword in self.keywords:
            keyword_results = [r for r in results if r.keyword == keyword]
            stats_data.append({
                'é—œéµå­—': keyword,
                'æ‰¾åˆ°æ•¸é‡': len(keyword_results),
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': np.mean([r.confidence for r in keyword_results]) if keyword_results else 0,
                'ä¸é‡è¤‡æ•¸å€¼': len(set([r.value for r in keyword_results]))
            })
        
        stats_df = pd.DataFrame(stats_data)
        
        # å¯«å…¥Excelæ–‡ä»¶
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='è©³ç´°çµæœ', index=False)
            stats_df.to_excel(writer, sheet_name='çµ±è¨ˆæ‘˜è¦', index=False)
        
        print(f"\nğŸ“Š Excelå ±å‘Šå·²ä¿å­˜è‡³: {excel_path}")
        return str(excel_path)

def main():
    """ä¸»å‡½æ•¸"""
    try:
        # åˆå§‹åŒ–æå–å™¨
        extractor = TwoStageExtractor()
        
        # åŸ·è¡Œå…©æ®µå¼æå–
        results = extractor.run_two_stage_extraction()
        
        if results:
            # ä¿å­˜çµæœ
            csv_path = extractor.save_results_to_csv(results)
            excel_path = extractor.save_results_to_excel(results)
            
            # é¡¯ç¤ºçµæœæ‘˜è¦
            print("\n" + "=" * 60)
            print("ğŸ‰ å…©æ®µå¼æå–å®Œæˆï¼")
            print("=" * 60)
            print(f"ğŸ“‹ ç¸½å…±æ‰¾åˆ°: {len(results)} å€‹åŒ…å«æ•¸å€¼çš„ç›¸é—œæ®µè½")
            
            # æŒ‰é—œéµå­—çµ±è¨ˆ
            for keyword in extractor.keywords:
                count = len([r for r in results if r.keyword == keyword])
                print(f"  ğŸ“Š {keyword}: {count} å€‹çµæœ")
            
            print(f"\nğŸ“ æª”æ¡ˆå·²ä¿å­˜:")
            print(f"  CSV: {csv_path}")
            print(f"  Excel: {excel_path}")
            
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¢ä»¶çš„çµæœ")
            
    except Exception as e:
        print(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()