import json
import hashlib
import os
import sys
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
import torch
import numpy as np
from tqdm import tqdm

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langchain_google_genai import ChatGoogleGenerativeAI

sys.path.append(str(Path(__file__).parent))
from config import *
from keywords_config import ESG_KEYWORDS_CONFIG, EXTRACTION_PROMPTS

@dataclass
class ExtractionResult:
    """æ•¸æ“šæå–çµæœ"""
    keyword: str
    indicator: str
    value: Optional[Union[float, str, bool]]
    value_type: str
    confidence: float
    source_text: str
    page_info: str

class ESGDataExtractor:
    def __init__(self, vector_db_path: str = None):
        """åˆå§‹åŒ–ESGæ•¸æ“šæå–å™¨"""
        
        if vector_db_path is None:
            vector_db_path = VECTOR_DB_PATH
            
        self.vector_db_path = vector_db_path
        self.keywords_config = ESG_KEYWORDS_CONFIG
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        print("è¼‰å…¥embeddingæ¨¡å‹...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        print("è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {vector_db_path}")
        
        self.db = FAISS.load_local(
            vector_db_path, 
            self.embedding_model, 
            allow_dangerous_deserialization=True
        )
        
        # åˆå§‹åŒ–reranker
        print("è¼‰å…¥rerankeræ¨¡å‹...")
        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL)
        self.reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL)
        
        # åˆå§‹åŒ–Gemini LLM
        print("åˆå§‹åŒ–Gemini LLM...")
        self.llm = ChatGoogleGenerativeAI(
            model=GEMINI_MODEL,
            google_api_key=GOOGLE_API_KEY,
            temperature=0,
            convert_system_message_to_human=True  # Geminiéœ€è¦é€™å€‹è¨­ç½®
        )
        
        print("âœ… ESGæ•¸æ“šæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _dedup_documents(self, documents: List[Document]) -> List[Document]:
        """å»é™¤é‡è¤‡æ–‡æª”"""
        seen_hashes = set()
        unique_docs = []
        for doc in documents:
            content_hash = hashlib.md5(doc.page_content.strip().encode("utf-8")).hexdigest()
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_docs.append(doc)
        return unique_docs

    def search_and_rerank(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """æœå°‹ä¸¦é‡æ–°æ’åºæ–‡æª”"""
        # åˆå§‹æœå°‹
        results = self.db.similarity_search(query, k=k*2)
        
        # å»é‡
        unique_results = self._dedup_documents(results)
        
        if len(unique_results) == 0:
            return []
        
        # ä½¿ç”¨BGE rerankeré‡æ–°æ’åº
        pairs = [(query, doc.page_content) for doc in unique_results]
        inputs = self.reranker_tokenizer.batch_encode_plus(
            pairs, padding=True, truncation=True, return_tensors="pt", max_length=512
        )
        
        with torch.no_grad():
            scores = self.reranker_model(**inputs).logits.view(-1)
        
        scores = scores.numpy()
        ranked = sorted(zip(unique_results, scores), key=lambda x: x[1], reverse=True)
        return ranked[:k]

    def extract_data_with_llm(self, keyword: str, context: str, data_type: str) -> ExtractionResult:
        """ä½¿ç”¨Gemini LLMæå–ç‰¹å®šæ•¸æ“š"""
        
        # é¸æ“‡å°æ‡‰çš„æç¤ºæ¨¡æ¿
        prompt_template = EXTRACTION_PROMPTS.get(data_type, EXTRACTION_PROMPTS["number"])
        prompt = prompt_template.format(keyword=keyword, context=context)
        
        try:
            # ç‚ºGeminiå„ªåŒ–prompt
            gemini_prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ESGæ•¸æ“šåˆ†æå¸«ã€‚è«‹ä»”ç´°åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸¦æå–ç›¸é—œä¿¡æ¯ã€‚

{prompt}

è«‹ç¢ºä¿å›ç­”æ ¼å¼ç‚ºæœ‰æ•ˆçš„JSONï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ã€‚
"""
            
            response = self.llm.invoke(gemini_prompt)
            
            # æ¸…ç†éŸ¿æ‡‰å…§å®¹ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            content = response.content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            result_json = json.loads(content)
            
            return ExtractionResult(
                keyword=keyword,
                indicator="",  # å°‡åœ¨èª¿ç”¨è™•è¨­ç½®
                value=result_json.get("value"),
                value_type=data_type,
                confidence=result_json.get("confidence", 0.0),
                source_text=context[:200] + "..." if len(context) > 200 else context,
                page_info=""  # å°‡åœ¨èª¿ç”¨è™•è¨­ç½®
            )
        except Exception as e:
            print(f"Geminiæå–å¤±æ•— ({keyword}): {e}")
            return ExtractionResult(
                keyword=keyword,
                indicator="",
                value="æå–å¤±æ•—",
                value_type="error",
                confidence=0.0,
                source_text=str(e),
                page_info=""
            )

    def extract_all_keywords(self) -> Dict[str, List[ExtractionResult]]:
        """æå–æ‰€æœ‰é—œéµå­—çš„æ•¸æ“š"""
        all_results = {}
        
        total_keywords = sum(len(config["keywords"]) for config in self.keywords_config.values())
        
        with tqdm(total=total_keywords, desc="æå–ESGæ•¸æ“š") as pbar:
            for indicator, config in self.keywords_config.items():
                print(f"\nğŸ“Š è™•ç†æŒ‡æ¨™: {indicator}")
                indicator_results = []
                
                for keyword in config["keywords"]:
                    pbar.set_description(f"æœå°‹: {keyword}")
                    
                    # æœå°‹ç›¸é—œæ–‡æª”
                    search_results = self.search_and_rerank(keyword, k=3)
                    
                    if not search_results:
                        # æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”
                        result = ExtractionResult(
                            keyword=keyword,
                            indicator=indicator,
                            value="å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°",
                            value_type="not_found",
                            confidence=1.0,
                            source_text="",
                            page_info=""
                        )
                        indicator_results.append(result)
                        pbar.update(1)
                        continue
                    
                    # åˆä½µç›¸é—œæ–‡æª”çš„å…§å®¹
                    combined_context = ""
                    page_info = []
                    for doc, score in search_results:
                        combined_context += doc.page_content + "\n\n"
                        page_info.append(f"ç¬¬{doc.metadata.get('page', 'unknown')}é ")
                    
                    # ä½¿ç”¨Geminiæå–æ•¸æ“š
                    result = self.extract_data_with_llm(
                        keyword, 
                        combined_context, 
                        config["type"]
                    )
                    result.indicator = indicator
                    result.page_info = ", ".join(page_info)
                    
                    indicator_results.append(result)
                    pbar.update(1)
                
                all_results[indicator] = indicator_results
        
        return all_results

    def generate_summary_report(self, results: Dict[str, List[ExtractionResult]]) -> Dict:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        summary = {
            "total_keywords": 0,
            "found_keywords": 0,
            "not_found_keywords": 0,
            "high_confidence_results": 0,
            "indicators_summary": {}
        }
        
        for indicator, indicator_results in results.items():
            indicator_summary = {
                "total": len(indicator_results),
                "found": 0,
                "not_found": 0,
                "high_confidence": 0,
                "key_findings": []
            }
            
            for result in indicator_results:
                summary["total_keywords"] += 1
                
                if result.value_type == "not_found":
                    summary["not_found_keywords"] += 1
                    indicator_summary["not_found"] += 1
                else:
                    summary["found_keywords"] += 1
                    indicator_summary["found"] += 1
                    
                    if result.confidence > 0.7:
                        summary["high_confidence_results"] += 1
                        indicator_summary["high_confidence"] += 1
                    
                    # æ”¶é›†é‡è¦ç™¼ç¾
                    if result.confidence > 0.6 and result.value != "æœªæåŠ":
                        indicator_summary["key_findings"].append({
                            "keyword": result.keyword,
                            "value": result.value,
                            "confidence": result.confidence
                        })
            
            summary["indicators_summary"][indicator] = indicator_summary
        
        return summary

    def save_results(self, results: Dict[str, List[ExtractionResult]], 
                    summary: Dict, output_path: str = None):
        """ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶"""
        
        if output_path is None:
            output_path = os.path.join(RESULTS_PATH, "esg_extraction_results.json")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # è½‰æ›ExtractionResultåˆ°å¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = {}
        for indicator, indicator_results in results.items():
            serializable_results[indicator] = [
                asdict(r) for r in indicator_results
            ]
        
        output_data = {
            "extraction_results": serializable_results,
            "summary": summary,
            "metadata": {
                "total_indicators": len(self.keywords_config),
                "extraction_method": "RAG + BGE Reranker + Gemini",
                "vector_db_path": self.vector_db_path,
                "model_used": GEMINI_MODEL
            }
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"çµæœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path