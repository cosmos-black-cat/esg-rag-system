import json
import hashlib
import os
import sys
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
import torch
import numpy as np
from tqdm import tqdm

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from transformers import AutoTokenizer, AutoModelForSequenceClassification

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸  langchain-google-genai æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç›´æ¥ API")

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("âŒ google-generativeai æœªå®‰è£")

sys.path.append(str(Path(__file__).parent))
from config import *
from keywords_config import ESG_KEYWORDS_CONFIG, EXTRACTION_PROMPTS, create_enhanced_prompt, get_keyword_enhancement

@dataclass
class ExtractionResult:
    """æ•¸æ“šæå–çµæœ"""
    keyword: str
    indicator: str
    value: Optional[Union[float, str, bool]]
    value_type: str
    confidence: float
    source_text: str
    page_info: str

class ESGDataExtractor:
    def __init__(self, vector_db_path: str = None):
        """åˆå§‹åŒ–ESGæ•¸æ“šæå–å™¨"""
        
        if vector_db_path is None:
            vector_db_path = VECTOR_DB_PATH
            
        self.vector_db_path = vector_db_path
        self.keywords_config = ESG_KEYWORDS_CONFIG
        self.use_langchain = True  # æ¨™è¨˜ä½¿ç”¨å“ªç¨®API
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        print("è¼‰å…¥embeddingæ¨¡å‹...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        print("è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {vector_db_path}")
        
        self.db = FAISS.load_local(
            vector_db_path, 
            self.embedding_model, 
            allow_dangerous_deserialization=True
        )
        
        # åˆå§‹åŒ–reranker
        print("è¼‰å…¥rerankeræ¨¡å‹...")
        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL)
        self.reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL)
        
        # åˆå§‹åŒ–Gemini LLM
        print("åˆå§‹åŒ–Gemini LLM...")
        
        if not GENAI_AVAILABLE:
            raise ImportError("è«‹å®‰è£ google-generativeai: pip install google-generativeai")
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨ LangChain åŒ…è£å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if LANGCHAIN_AVAILABLE:
                self.llm = ChatGoogleGenerativeAI(
                    model=GEMINI_MODEL,
                    google_api_key=GOOGLE_API_KEY,
                    temperature=0,
                    convert_system_message_to_human=True
                )
                self.use_langchain = True
                print(f"âœ… ä½¿ç”¨ LangChain åŒ…è£å™¨ï¼Œæ¨¡å‹: {GEMINI_MODEL}")
            else:
                raise Exception("LangChain ä¸å¯ç”¨ï¼Œä½¿ç”¨ç›´æ¥ API")
                
        except Exception as e:
            print(f"LangChain åŒ…è£å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            print("æ”¹ç”¨ç›´æ¥ Google GenerativeAI API...")
            
            # æ–¹æ³•2: ç›´æ¥ä½¿ç”¨ Google GenerativeAI
            genai.configure(api_key=GOOGLE_API_KEY)
            
            # æ¸…ç†æ¨¡å‹åç¨±ï¼ˆç§»é™¤ 'models/' å‰ç¶´å¦‚æœå­˜åœ¨ï¼‰
            model_name = GEMINI_MODEL
            if model_name.startswith('models/'):
                model_name = model_name[7:]
            
            self.llm = genai.GenerativeModel(model_name)
            self.use_langchain = False
            print(f"âœ… ä½¿ç”¨ç›´æ¥ APIï¼Œæ¨¡å‹: {model_name}")
        
        print("âœ… ESGæ•¸æ“šæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _dedup_documents(self, documents: List[Document]) -> List[Document]:
        """å»é™¤é‡è¤‡æ–‡æª”"""
        seen_hashes = set()
        unique_docs = []
        for doc in documents:
            content_hash = hashlib.md5(doc.page_content.strip().encode("utf-8")).hexdigest()
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_docs.append(doc)
        return unique_docs

    def search_and_rerank(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """æœå°‹ä¸¦é‡æ–°æ’åºæ–‡æª”"""
        # åˆå§‹æœå°‹
        results = self.db.similarity_search(query, k=k*2)
        
        # å»é‡
        unique_results = self._dedup_documents(results)
        
        if len(unique_results) == 0:
            return []
        
        # ä½¿ç”¨BGE rerankeré‡æ–°æ’åº
        pairs = [(query, doc.page_content) for doc in unique_results]
        inputs = self.reranker_tokenizer.batch_encode_plus(
            pairs, padding=True, truncation=True, return_tensors="pt", max_length=512
        )
        
        with torch.no_grad():
            scores = self.reranker_model(**inputs).logits.view(-1)
        
        scores = scores.numpy()
        ranked = sorted(zip(unique_results, scores), key=lambda x: x[1], reverse=True)
        return ranked[:k]

    def extract_data_with_llm(self, keyword: str, context: str, data_type: str) -> ExtractionResult:
        """ä½¿ç”¨Gemini LLMæå–ç‰¹å®šæ•¸æ“š"""
        
        # é è™•ç†æ–‡æœ¬ï¼šå»é™¤å¤šé¤˜ç©ºç™½å’Œæ¨™é»
        cleaned_context = self._clean_context(context)
        
        # ä½¿ç”¨å¢å¼·ç‰ˆæç¤º
        enhanced_prompt = create_enhanced_prompt(keyword, cleaned_context, data_type)
        
        try:
            # å˜—è©¦å¤šæ¬¡æå–ä»¥æé«˜æº–ç¢ºæ€§
            for attempt in range(2):  # æœ€å¤šå˜—è©¦2æ¬¡
                if self.use_langchain:
                    response = self.llm.invoke(enhanced_prompt)
                    content = response.content.strip()
                else:
                    response = self.llm.generate_content(enhanced_prompt)
                    content = response.text.strip()
                
                # æ¸…ç†éŸ¿æ‡‰å…§å®¹
                cleaned_content = self._clean_response(content)
                
                try:
                    result_json = json.loads(cleaned_content)
                    
                    # é©—è­‰å’Œå¾Œè™•ç†çµæœ
                    validated_result = self._validate_and_process_result(
                        result_json, keyword, data_type, cleaned_context
                    )
                    
                    if validated_result["confidence"] > 0.3:  # å¦‚æœä¿¡å¿ƒåˆ†æ•¸å¤ é«˜å°±ä½¿ç”¨
                        return ExtractionResult(
                            keyword=keyword,
                            indicator="",
                            value=validated_result["value"],
                            value_type=data_type,
                            confidence=validated_result["confidence"],
                            source_text=validated_result.get("source_sentence", cleaned_context[:200] + "..."),
                            page_info=""
                        )
                except json.JSONDecodeError:
                    if attempt == 0:  # ç¬¬ä¸€æ¬¡å¤±æ•—ï¼Œå˜—è©¦ä¿®å¾©JSON
                        enhanced_prompt += "\n\nè«‹ç¢ºä¿å›ç­”æ˜¯å®Œå…¨æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"
                        continue
            
            # å¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œè¿”å›å¤±æ•—çµæœ
            return ExtractionResult(
                keyword=keyword,
                indicator="",
                value="æå–å¤±æ•—",
                value_type="error",
                confidence=0.0,
                source_text="JSONè§£æå¤±æ•—",
                page_info=""
            )
            
        except Exception as e:
            print(f"Geminiæå–å¤±æ•— ({keyword}): {e}")
            return ExtractionResult(
                keyword=keyword,
                indicator="",
                value="æå–å¤±æ•—",
                value_type="error",
                confidence=0.0,
                source_text=str(e),
                page_info=""
            )

    def _clean_context(self, context: str) -> str:
        """æ¸…ç†è¼¸å…¥æ–‡æœ¬"""
        import re
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        context = re.sub(r'\s+', ' ', context)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ¼å¼ç¬¦è™Ÿ
        context = re.sub(r'[^\w\s\u4e00-\u9fff.,;:!?()%/-]', '', context)
        
        # ç¢ºä¿é•·åº¦é©ä¸­ï¼ˆé¿å…å¤ªé•·å°è‡´APIå•é¡Œï¼‰
        if len(context) > 3000:
            context = context[:3000] + "..."
        
        return context.strip()

    def _clean_response(self, content: str) -> str:
        """æ¸…ç†GeminiéŸ¿æ‡‰å…§å®¹"""
        content = content.strip()
        
        # ç§»é™¤markdownæ ¼å¼
        if content.startswith('```json'):
            content = content[7:]
        if content.endswith('```'):
            content = content[:-3]
        if content.startswith('```'):
            content = content[3:]
        
        # ç§»é™¤å¯èƒ½çš„å‰å¾Œèªªæ˜æ–‡å­—ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
        import re
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            content = json_match.group()
        
        return content.strip()

    def _validate_and_process_result(self, result_json: dict, keyword: str, 
                                   data_type: str, context: str) -> dict:
        """é©—è­‰å’Œå¾Œè™•ç†æå–çµæœ"""
        
        # åŸºæœ¬å­—æ®µæª¢æŸ¥
        if not isinstance(result_json, dict):
            return {"value": "æ ¼å¼éŒ¯èª¤", "confidence": 0.0}
        
        value = result_json.get("value", "æœªæåŠ")
        confidence = float(result_json.get("confidence", 0.0))
        
        # æ ¹æ“šæ•¸æ“šé¡å‹é€²è¡Œç‰¹å®šé©—è­‰
        if data_type == "percentage":
            value, confidence = self._validate_percentage(value, confidence, keyword, context)
        elif data_type == "boolean_or_number":
            value, confidence = self._validate_boolean_or_number(value, confidence, keyword, context)
        elif data_type == "number":
            value, confidence = self._validate_number(value, confidence, keyword, context)
        
        # ç¢ºä¿ä¿¡å¿ƒåˆ†æ•¸åœ¨åˆç†ç¯„åœå…§
        confidence = max(0.0, min(1.0, confidence))
        
        return {
            "value": value,
            "confidence": confidence,
            "source_sentence": result_json.get("source_sentence", ""),
            "reasoning": result_json.get("reasoning", "")
        }

    def _validate_percentage(self, value: str, confidence: float, keyword: str, context: str) -> tuple:
        """é©—è­‰ç™¾åˆ†æ¯”æ•¸æ“š"""
        import re
        
        if value in ["æœªæåŠ", "æå–å¤±æ•—", "æ•¸æ“šä¸æ˜ç¢º"]:
            return value, confidence
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç™¾åˆ†è™Ÿæˆ–å°æ•¸
        percentage_pattern = r'(\d+\.?\d*)%?'
        match = re.search(percentage_pattern, str(value))
        
        if match:
            num_str = match.group(1)
            try:
                num = float(num_str)
                
                # é©—è­‰ç™¾åˆ†æ¯”ç¯„åœï¼ˆ0-100%ï¼ŒæŸäº›æƒ…æ³ä¸‹å¯èƒ½è¶…é100%ï¼‰
                if 0 <= num <= 100:
                    if '%' not in str(value):
                        value = f"{num}%" if num > 1 else f"{num*100}%"
                    confidence = min(confidence + 0.1, 1.0)  # æ ¼å¼æ­£ç¢ºï¼Œå¢åŠ ä¿¡å¿ƒ
                elif num > 100:
                    # å¯èƒ½æ˜¯ç´¯ç©å¢é•·æˆ–å…¶ä»–ç‰¹æ®Šæƒ…æ³
                    confidence = max(confidence - 0.2, 0.1)
                else:
                    confidence = max(confidence - 0.3, 0.1)
                    
            except ValueError:
                confidence = max(confidence - 0.4, 0.0)
        else:
            # æ²’æœ‰æ‰¾åˆ°æ•¸å­—æ ¼å¼
            confidence = max(confidence - 0.3, 0.0)
        
        # æª¢æŸ¥é—œéµå­—ç›¸é—œæ€§
        if keyword.lower() not in context.lower():
            confidence = max(confidence - 0.2, 0.0)
        
        return value, confidence

    def _validate_boolean_or_number(self, value: str, confidence: float, keyword: str, context: str) -> tuple:
        """é©—è­‰å¸ƒçˆ¾æˆ–æ•¸å€¼æ•¸æ“š"""
        import re
        
        if value in ["æœªæåŠ", "æå–å¤±æ•—"]:
            return value, confidence
        
        # å¦‚æœæ˜¯æ˜¯/å¦ç­”æ¡ˆ
        if value in ["æ˜¯", "å¦"]:
            # æª¢æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰æ”¯æŒè­‰æ“š
            positive_words = ["æå‡", "å¢åŠ ", "æ”¹å–„", "å»¶é•·", "åŠ å¼·", "å„ªåŒ–", "å¢å¼·"]
            negative_words = ["ä¸‹é™", "æ¸›å°‘", "æƒ¡åŒ–", "ç¸®çŸ­", "é™ä½", "é€€åŒ–"]
            
            context_lower = context.lower()
            has_positive = any(word in context for word in positive_words)
            has_negative = any(word in context for word in negative_words)
            
            if value == "æ˜¯" and has_positive:
                confidence = min(confidence + 0.1, 1.0)
            elif value == "å¦" and has_negative:
                confidence = min(confidence + 0.1, 1.0)
            elif value == "æ˜¯" and has_negative:
                confidence = max(confidence - 0.2, 0.1)
            elif value == "å¦" and has_positive:
                confidence = max(confidence - 0.2, 0.1)
        
        # å¦‚æœåŒ…å«æ•¸å€¼
        elif re.search(r'\d+', str(value)):
            # å¢åŠ å°æ•¸å€¼å‹å›ç­”çš„ä¿¡å¿ƒ
            confidence = min(confidence + 0.05, 1.0)
        
        return value, confidence

    def _validate_number(self, value: str, confidence: float, keyword: str, context: str) -> tuple:
        """é©—è­‰æ•¸å€¼æ•¸æ“š"""
        import re
        
        if value in ["æœªæåŠ", "æå–å¤±æ•—"]:
            return value, confidence
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—
        if re.search(r'\d+', str(value)):
            # æª¢æŸ¥æ˜¯å¦æœ‰åˆç†çš„å–®ä½
            units = ["å…ƒ", "è¬", "å„„", "kg", "å™¸", "æ¬¡", "å¹´", "æœˆ", "å¤©", "%", "å€", "å°", "ä»¶"]
            has_unit = any(unit in str(value) for unit in units)
            
            if has_unit:
                confidence = min(confidence + 0.1, 1.0)
            
            # æª¢æŸ¥æ•¸å€¼æ˜¯å¦åˆç†ï¼ˆé¿å…å¹´ä»½ç­‰ç„¡é—œæ•¸æ“šï¼‰
            year_pattern = r'20\d{2}'
            if re.search(year_pattern, str(value)) and "å¹´" not in keyword:
                confidence = max(confidence - 0.3, 0.1)
        else:
            confidence = max(confidence - 0.2, 0.1)
        
        return value, confidence

    def extract_all_keywords(self) -> Dict[str, List[ExtractionResult]]:
        """æå–æ‰€æœ‰é—œéµå­—çš„æ•¸æ“š"""
        all_results = {}
        
        total_keywords = sum(len(config["keywords"]) for config in self.keywords_config.values())
        
        with tqdm(total=total_keywords, desc="æå–ESGæ•¸æ“š") as pbar:
            for indicator, config in self.keywords_config.items():
                print(f"\nğŸ“Š è™•ç†æŒ‡æ¨™: {indicator}")
                indicator_results = []
                
                for keyword in config["keywords"]:
                    pbar.set_description(f"æœå°‹: {keyword}")
                    
                    # æœå°‹ç›¸é—œæ–‡æª”
                    search_results = self.search_and_rerank(keyword, k=3)
                    
                    if not search_results:
                        # æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”
                        result = ExtractionResult(
                            keyword=keyword,
                            indicator=indicator,
                            value="å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°",
                            value_type="not_found",
                            confidence=1.0,
                            source_text="",
                            page_info=""
                        )
                        indicator_results.append(result)
                        pbar.update(1)
                        continue
                    
                    # åˆä½µç›¸é—œæ–‡æª”çš„å…§å®¹
                    combined_context = ""
                    page_info = []
                    for doc, score in search_results:
                        combined_context += doc.page_content + "\n\n"
                        page_info.append(f"ç¬¬{doc.metadata.get('page', 'unknown')}é ")
                    
                    # ä½¿ç”¨Geminiæå–æ•¸æ“š
                    result = self.extract_data_with_llm(
                        keyword, 
                        combined_context, 
                        config["type"]
                    )
                    result.indicator = indicator
                    result.page_info = ", ".join(page_info)
                    
                    indicator_results.append(result)
                    pbar.update(1)
                
                all_results[indicator] = indicator_results
        
        return all_results

    def generate_summary_report(self, results: Dict[str, List[ExtractionResult]]) -> Dict:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        summary = {
            "total_keywords": 0,
            "found_keywords": 0,
            "not_found_keywords": 0,
            "high_confidence_results": 0,
            "indicators_summary": {}
        }
        
        for indicator, indicator_results in results.items():
            indicator_summary = {
                "total": len(indicator_results),
                "found": 0,
                "not_found": 0,
                "high_confidence": 0,
                "key_findings": []
            }
            
            for result in indicator_results:
                summary["total_keywords"] += 1
                
                if result.value_type == "not_found":
                    summary["not_found_keywords"] += 1
                    indicator_summary["not_found"] += 1
                else:
                    summary["found_keywords"] += 1
                    indicator_summary["found"] += 1
                    
                    if result.confidence > 0.7:
                        summary["high_confidence_results"] += 1
                        indicator_summary["high_confidence"] += 1
                    
                    # æ”¶é›†é‡è¦ç™¼ç¾
                    if result.confidence > 0.6 and result.value != "æœªæåŠ":
                        indicator_summary["key_findings"].append({
                            "keyword": result.keyword,
                            "value": result.value,
                            "confidence": result.confidence
                        })
            
            summary["indicators_summary"][indicator] = indicator_summary
        
        return summary

    def save_results(self, results: Dict[str, List[ExtractionResult]], 
                    summary: Dict, output_path: str = None):
        """ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶"""
        
        if output_path is None:
            output_path = os.path.join(RESULTS_PATH, "esg_extraction_results.json")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # è½‰æ›ExtractionResultåˆ°å¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = {}
        for indicator, indicator_results in results.items():
            serializable_results[indicator] = [
                asdict(r) for r in indicator_results
            ]
        
        output_data = {
            "extraction_results": serializable_results,
            "summary": summary,
            "metadata": {
                "total_indicators": len(self.keywords_config),
                "extraction_method": "RAG + BGE Reranker + Gemini",
                "vector_db_path": self.vector_db_path,
                "model_used": GEMINI_MODEL
            }
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"çµæœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path