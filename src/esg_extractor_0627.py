import json
import hashlib
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Union, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import torch
import numpy as np
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from transformers import AutoTokenizer, AutoModelForSequenceClassification

sys.path.append(str(Path(__file__).parent))
from config import *
from keywords_config import ESG_KEYWORDS_CONFIG, EXTRACTION_PROMPTS
from api_manager import GeminiAPIManager, GEMINI_API_KEYS

@dataclass
class ExtractionResult:
    """æ•¸æ“šæå–çµæœ"""
    keyword: str
    indicator: str
    value: Optional[Union[float, str, bool]]
    value_type: str
    confidence: float
    source_text: str
    page_info: str
    explanation: str = ""
    api_key_used: str = ""  # è¨˜éŒ„ä½¿ç”¨çš„API key

@dataclass
class SimilarKeywordGroup:
    """ç›¸ä¼¼é—œéµå­—çµ„"""
    keywords: List[str]
    similarity_score: float
    common_value: str
    confidence_avg: float

class MultiKeyESGDataExtractor:
    def __init__(self, vector_db_path: str = None):
        """åˆå§‹åŒ–ESGæ•¸æ“šæå–å™¨ - æ”¯æŒå¤šAPI key"""
        
        if vector_db_path is None:
            vector_db_path = VECTOR_DB_PATH
            
        self.vector_db_path = vector_db_path
        self.keywords_config = ESG_KEYWORDS_CONFIG
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        print("è¼‰å…¥embeddingæ¨¡å‹...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        
        # è¼‰å…¥å‘é‡è³‡æ–™åº«
        print("è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(vector_db_path):
            raise FileNotFoundError(f"å‘é‡è³‡æ–™åº«ä¸å­˜åœ¨: {vector_db_path}")
        
        self.db = FAISS.load_local(
            vector_db_path, 
            self.embedding_model, 
            allow_dangerous_deserialization=True
        )
        
        # åˆå§‹åŒ–reranker
        print("è¼‰å…¥rerankeræ¨¡å‹...")
        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL)
        self.reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL)
        
        # åˆå§‹åŒ–å¤šAPI keyç®¡ç†å™¨
        print("åˆå§‹åŒ–å¤šAPI key Geminiç®¡ç†å™¨...")
        self.api_manager = GeminiAPIManager(
            api_keys=GEMINI_API_KEYS,
            model_name=GEMINI_MODEL
        )
        
        print("âœ… å¤šAPI key ESGæ•¸æ“šæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _dedup_documents(self, documents: List[Document]) -> List[Document]:
        """å»é™¤é‡è¤‡æ–‡æª”"""
        seen_hashes = set()
        unique_docs = []
        for doc in documents:
            content_hash = hashlib.md5(doc.page_content.strip().encode("utf-8")).hexdigest()
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_docs.append(doc)
        return unique_docs

    def search_and_rerank(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """æœå°‹ä¸¦é‡æ–°æ’åºæ–‡æª”"""
        try:
            # åˆå§‹æœå°‹
            results = self.db.similarity_search(query, k=k*2)
            
            # å»é‡
            unique_results = self._dedup_documents(results)
            
            if len(unique_results) == 0:
                return []
            
            # ä½¿ç”¨BGE rerankeré‡æ–°æ’åº
            pairs = [(query, doc.page_content) for doc in unique_results]
            inputs = self.reranker_tokenizer.batch_encode_plus(
                pairs, padding=True, truncation=True, return_tensors="pt", max_length=512
            )
            
            with torch.no_grad():
                scores = self.reranker_model(**inputs).logits.view(-1)
            
            scores = scores.numpy()
            ranked = sorted(zip(unique_results, scores), key=lambda x: x[1], reverse=True)
            return ranked[:k]
        except Exception as e:
            print(f"æœå°‹éŒ¯èª¤ ({query}): {e}")
            return []

    def extract_data_with_llm(self, keyword: str, context: str, data_type: str) -> ExtractionResult:
        """ä½¿ç”¨å¤šAPI key Geminiæå–ç‰¹å®šæ•¸æ“š"""
        
        # é¸æ“‡å°æ‡‰çš„æç¤ºæ¨¡æ¿
        prompt_template = EXTRACTION_PROMPTS.get(data_type, EXTRACTION_PROMPTS["number"])
        prompt = prompt_template.format(keyword=keyword, context=context)
        
        # ç‚ºGeminiå„ªåŒ–prompt
        gemini_prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ESGæ•¸æ“šåˆ†æå¸«ã€‚è«‹ä»”ç´°åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸¦æå–ç›¸é—œä¿¡æ¯ã€‚

{prompt}

é‡è¦æé†’ï¼š
1. å¿…é ˆçµ¦å‡ºä¸€å€‹æ˜ç¢ºçš„ç­”æ¡ˆï¼Œä¸èƒ½å›ç­”"ä¸çŸ¥é“"æˆ–"ç„¡æ³•ç¢ºå®š"
2. å¦‚æœæ–‡æœ¬ä¸­æ²’æœ‰ç›´æ¥æåˆ°è©²é—œéµå­—ï¼Œä½†æœ‰ç›¸é—œæ¦‚å¿µï¼Œè«‹åŸºæ–¼ç›¸é—œä¿¡æ¯æ¨æ–·
3. å¦‚æœå®Œå…¨æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºå›ç­”"æœªæåŠ"
4. è«‹ç¢ºä¿å›ç­”æ ¼å¼ç‚ºæœ‰æ•ˆçš„JSON

è«‹ä»¥JSONæ ¼å¼å›ç­”ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ã€‚
"""
        
        try:
            # ä½¿ç”¨APIç®¡ç†å™¨èª¿ç”¨
            print(f"ğŸ¤– æ­£åœ¨æå–é—œéµå­—: {keyword}")
            response = self.api_manager.invoke(gemini_prompt)
            
            # æ¸…ç†éŸ¿æ‡‰å…§å®¹ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            content = response.content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            result_json = json.loads(content)
            
            # ç²å–ç•¶å‰ä½¿ç”¨çš„API keyä¿¡æ¯
            current_key = self.api_manager.api_keys[self.api_manager.current_key_index]
            
            return ExtractionResult(
                keyword=keyword,
                indicator="",  # å°‡åœ¨èª¿ç”¨è™•è¨­ç½®
                value=result_json.get("value", "æœªæåŠ"),
                value_type=data_type,
                confidence=result_json.get("confidence", 0.8),
                source_text=context[:200] + "..." if len(context) > 200 else context,
                page_info="",  # å°‡åœ¨èª¿ç”¨è™•è¨­ç½®
                explanation=result_json.get("explanation", ""),
                api_key_used=current_key[:10] + "..."
            )
            
        except Exception as e:
            print(f"âŒ LLMæå–å¤±æ•— ({keyword}): {e}")
            # å³ä½¿LLMæå–å¤±æ•—ï¼Œä¹Ÿè¦çµ¦å‡ºä¸€å€‹ç­”æ¡ˆ
            return ExtractionResult(
                keyword=keyword,
                indicator="",
                value="æœªæåŠ",
                value_type=data_type,
                confidence=0.5,
                source_text="LLMæå–éç¨‹ä¸­å‡ºç¾éŒ¯èª¤",
                page_info="",
                explanation=f"æå–éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {str(e)}",
                api_key_used="error"
            )

    def extract_keyword_data(self, keyword: str, indicator: str, data_type: str) -> ExtractionResult:
        """å°å–®å€‹é—œéµå­—é€²è¡Œæ•¸æ“šæå–ï¼Œç¢ºä¿æœ‰ç­”æ¡ˆ"""
        
        # 1. æœå°‹ç›¸é—œæ–‡æª”
        search_results = self.search_and_rerank(keyword, k=3)
        
        if not search_results:
            # æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”ï¼Œä½†é‚„æ˜¯è¦çµ¦å‡ºç­”æ¡ˆ
            return ExtractionResult(
                keyword=keyword,
                indicator=indicator,
                value="å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°",
                value_type=data_type,
                confidence=1.0,
                source_text="æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”",
                page_info="",
                explanation="åœ¨å ±å‘Šæ›¸ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤é—œéµå­—ç›¸é—œçš„å…§å®¹",
                api_key_used="no_search"
            )
        
        # 2. åˆä½µç›¸é—œæ–‡æª”çš„å…§å®¹
        combined_context = ""
        page_info = []
        for doc, score in search_results:
            combined_context += doc.page_content + "\n\n"
            page_info.append(f"ç¬¬{doc.metadata.get('page', 'unknown')}é ")
        
        # 3. ä½¿ç”¨å¤šAPI key Geminiæå–æ•¸æ“š
        result = self.extract_data_with_llm(keyword, combined_context, data_type)
        result.indicator = indicator
        result.page_info = ", ".join(page_info)
        
        return result

    def extract_all_keywords(self) -> Dict[str, List[ExtractionResult]]:
        """æå–æ‰€æœ‰é—œéµå­—çš„æ•¸æ“šï¼Œä½¿ç”¨å¤šAPI keyç®¡ç†"""
        all_results = {}
        
        total_keywords = sum(len(config["keywords"]) for config in self.keywords_config.values())
        
        print(f"\nğŸ” é–‹å§‹æå– {total_keywords} å€‹é—œéµå­—çš„æ•¸æ“š...")
        print(f"ğŸ”‘ ä½¿ç”¨ {len(GEMINI_API_KEYS)} å€‹API keyè¼ªæ›")
        
        with tqdm(total=total_keywords, desc="æå–ESGæ•¸æ“š") as pbar:
            for indicator, config in self.keywords_config.items():
                print(f"\nğŸ“Š è™•ç†æŒ‡æ¨™: {indicator}")
                indicator_results = []
                
                for keyword in config["keywords"]:
                    pbar.set_description(f"æå–: {keyword}")
                    
                    # å°æ¯å€‹é—œéµå­—é€²è¡Œæå–
                    result = self.extract_keyword_data(keyword, indicator, config["type"])
                    indicator_results.append(result)
                    
                    # é¡¯ç¤ºé€²åº¦å’Œä½¿ç”¨çš„API key
                    status = "âœ“" if result.value != "æœªæåŠ" and result.value != "å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°" else "â—‹"
                    print(f"  {status} {keyword}: {result.value} (key: {result.api_key_used})")
                    
                    pbar.update(1)
                
                all_results[indicator] = indicator_results
                
                # æ¯å®Œæˆä¸€å€‹æŒ‡æ¨™ï¼Œé¡¯ç¤ºAPIä½¿ç”¨çµ±è¨ˆ
                if len(indicator_results) > 5:  # é¿å…å¤ªé »ç¹çš„çµ±è¨ˆè¼¸å‡º
                    print(f"ğŸ“ˆ å·²å®Œæˆ {indicator}ï¼ŒAPIä½¿ç”¨çµ±è¨ˆ:")
                    self._print_brief_api_stats()
        
        return all_results

    def _print_brief_api_stats(self):
        """ç°¡è¦æ‰“å°APIä½¿ç”¨çµ±è¨ˆ"""
        stats = self.api_manager.get_usage_statistics()
        total = stats["total_requests"]
        
        key_usage = []
        for key_name, key_stats in stats["keys_usage"].items():
            usage = key_stats["usage_count"]
            if usage > 0:
                key_usage.append(f"{key_name}:{usage}")
        
        print(f"   ğŸ“Š ç¸½è«‹æ±‚: {total} | " + " | ".join(key_usage))

    def find_similar_keywords(self, results: Dict[str, List[ExtractionResult]], 
                            top_n: int = 10) -> List[SimilarKeywordGroup]:
        """æ‰¾å‡ºç›¸ä¼¼åº¦æœ€é«˜çš„é—œéµå­—çµ„åˆ"""
        print("\nğŸ” åˆ†æé—œéµå­—ç›¸ä¼¼åº¦...")
        
        # æ”¶é›†æ‰€æœ‰çµæœ
        all_results = []
        for indicator_results in results.values():
            all_results.extend(indicator_results)
        
        # éæ¿¾å‡ºæœ‰æ•ˆçµæœï¼ˆé"æœªæåŠ"ï¼‰
        valid_results = [r for r in all_results 
                        if r.value not in ["æœªæåŠ", "å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°"] 
                        and r.confidence > 0.6]
        
        if len(valid_results) < 2:
            return []
        
        # å‰µå»ºé—œéµå­—å’Œå€¼çš„é…å°
        keywords = [r.keyword for r in valid_results]
        values = [str(r.value) for r in valid_results]
        
        # ä½¿ç”¨TF-IDFè¨ˆç®—é—œéµå­—ç›¸ä¼¼åº¦
        vectorizer = TfidfVectorizer()
        try:
            keyword_vectors = vectorizer.fit_transform(keywords)
            similarity_matrix = cosine_similarity(keyword_vectors)
        except:
            # å¦‚æœTF-IDFå¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            similarity_matrix = np.zeros((len(keywords), len(keywords)))
            for i in range(len(keywords)):
                for j in range(len(keywords)):
                    if i != j:
                        # ç°¡å–®çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                        k1, k2 = keywords[i].lower(), keywords[j].lower()
                        common_chars = len(set(k1) & set(k2))
                        total_chars = len(set(k1) | set(k2))
                        similarity_matrix[i][j] = common_chars / total_chars if total_chars > 0 else 0
        
        # æ‰¾å‡ºç›¸ä¼¼åº¦æœ€é«˜çš„çµ„åˆ
        similar_groups = []
        processed_pairs = set()
        
        for i in range(len(keywords)):
            for j in range(i+1, len(keywords)):
                if (i, j) in processed_pairs:
                    continue
                
                similarity = similarity_matrix[i][j]
                if similarity > 0.3:  # ç›¸ä¼¼åº¦é–¾å€¼
                    # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒæˆ–ç›¸ä¼¼çš„å€¼
                    val1, val2 = values[i], values[j]
                    if val1 == val2 or self._values_similar(val1, val2):
                        group = SimilarKeywordGroup(
                            keywords=[keywords[i], keywords[j]],
                            similarity_score=similarity,
                            common_value=val1,
                            confidence_avg=(valid_results[i].confidence + valid_results[j].confidence) / 2
                        )
                        similar_groups.append(group)
                        processed_pairs.add((i, j))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºä¸¦è¿”å›å‰Nå€‹
        similar_groups.sort(key=lambda x: x.similarity_score, reverse=True)
        return similar_groups[:top_n]

    def _values_similar(self, val1: str, val2: str) -> bool:
        """åˆ¤æ–·å…©å€‹å€¼æ˜¯å¦ç›¸ä¼¼"""
        try:
            # å˜—è©¦æå–æ•¸å­—
            import re
            nums1 = re.findall(r'\d+(?:\.\d+)?', str(val1))
            nums2 = re.findall(r'\d+(?:\.\d+)?', str(val2))
            
            if nums1 and nums2:
                # å¦‚æœéƒ½æœ‰æ•¸å­—ï¼Œæ¯”è¼ƒç¬¬ä¸€å€‹æ•¸å­—
                return abs(float(nums1[0]) - float(nums2[0])) < 0.1
        except:
            pass
        
        return str(val1).lower() == str(val2).lower()

    def generate_indicator_statistics(self, results: Dict[str, List[ExtractionResult]]) -> pd.DataFrame:
        """ç”Ÿæˆå„æŒ‡æ¨™çµ±è¨ˆ"""
        print("\nğŸ“Š ç”ŸæˆæŒ‡æ¨™çµ±è¨ˆ...")
        
        stats_data = []
        
        for indicator, indicator_results in results.items():
            total_keywords = len(indicator_results)
            found_keywords = len([r for r in indicator_results 
                                if r.value not in ["æœªæåŠ", "å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°"]])
            not_found_keywords = total_keywords - found_keywords
            success_rate = (found_keywords / total_keywords * 100) if total_keywords > 0 else 0
            
            # é«˜ä¿¡å¿ƒçµæœ
            high_confidence = len([r for r in indicator_results if r.confidence > 0.8])
            
            # å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
            avg_confidence = np.mean([r.confidence for r in indicator_results])
            
            # é—œéµç™¼ç¾æ•¸é‡
            key_findings = len([r for r in indicator_results 
                              if r.confidence > 0.7 and r.value not in ["æœªæåŠ", "å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°"]])
            
            # API keyä½¿ç”¨çµ±è¨ˆ
            api_key_usage = {}
            for result in indicator_results:
                key = result.api_key_used
                api_key_usage[key] = api_key_usage.get(key, 0) + 1
            
            most_used_key = max(api_key_usage.items(), key=lambda x: x[1])[0] if api_key_usage else "unknown"
            
            stats_data.append({
                'æŒ‡æ¨™åç¨±': indicator,
                'ç¸½é—œéµå­—æ•¸': total_keywords,
                'æˆåŠŸæå–æ•¸': found_keywords,
                'æœªæ‰¾åˆ°æ•¸': not_found_keywords,
                'æˆåŠŸç‡(%)': round(success_rate, 1),
                'é«˜ä¿¡å¿ƒçµæœæ•¸': high_confidence,
                'å¹³å‡ä¿¡å¿ƒåˆ†æ•¸': round(avg_confidence, 3),
                'é—œéµç™¼ç¾æ•¸': key_findings,
                'ä¸»è¦ä½¿ç”¨API': most_used_key
            })
        
        return pd.DataFrame(stats_data)

    def generate_excel_report(self, results: Dict[str, List[ExtractionResult]], 
                            output_path: str = None) -> str:
        """ç”ŸæˆåŒ…å«å¤šå€‹å·¥ä½œè¡¨çš„Excelå ±å‘Š"""
        
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = os.path.join(RESULTS_PATH, f"esg_multikey_report_{timestamp}.xlsx")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        print(f"\nğŸ“Š ç”ŸæˆExcelå ±å‘Š: {output_path}")
        
        # å‰µå»ºExcel Writer
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            
            # å·¥ä½œè¡¨1: å®Œæ•´æå–çµæœ
            print("  ğŸ“‹ ç”Ÿæˆå·¥ä½œè¡¨1: å®Œæ•´æå–çµæœ")
            all_data = []
            for indicator, indicator_results in results.items():
                for result in indicator_results:
                    all_data.append({
                        'æŒ‡æ¨™é¡åˆ¥': result.indicator,
                        'é—œéµå­—': result.keyword,
                        'æå–å€¼': str(result.value),
                        'æ•¸æ“šé¡å‹': result.value_type,
                        'ä¿¡å¿ƒåˆ†æ•¸': round(result.confidence, 3),
                        'ä¾†æºé é¢': result.page_info,
                        'ä¾†æºæ–‡æœ¬': result.source_text[:100] + "..." if len(result.source_text) > 100 else result.source_text,
                        'èªªæ˜': result.explanation,
                        'ä½¿ç”¨çš„API Key': result.api_key_used
                    })
            
            df_all = pd.DataFrame(all_data)
            df_all.to_excel(writer, sheet_name='å®Œæ•´æå–çµæœ', index=False)
            
            # å·¥ä½œè¡¨2: ç›¸ä¼¼é—œéµå­—çµæœ
            print("  ğŸ”— ç”Ÿæˆå·¥ä½œè¡¨2: ç›¸ä¼¼é—œéµå­—çµæœ")
            similar_groups = self.find_similar_keywords(results)
            
            if similar_groups:
                similar_data = []
                for i, group in enumerate(similar_groups, 1):
                    for keyword in group.keywords:
                        # æ‰¾åˆ°å°æ‡‰çš„çµæœ
                        for indicator_results in results.values():
                            for result in indicator_results:
                                if result.keyword == keyword:
                                    similar_data.append({
                                        'çµ„åˆ¥': f"ç›¸ä¼¼çµ„{i}",
                                        'ç›¸ä¼¼åº¦åˆ†æ•¸': round(group.similarity_score, 3),
                                        'å…±åŒå€¼': group.common_value,
                                        'æŒ‡æ¨™é¡åˆ¥': result.indicator,
                                        'é—œéµå­—': result.keyword,
                                        'æå–å€¼': str(result.value),
                                        'ä¿¡å¿ƒåˆ†æ•¸': round(result.confidence, 3),
                                        'ä¾†æºé é¢': result.page_info,
                                        'ä½¿ç”¨çš„API Key': result.api_key_used
                                    })
                                    break
                
                df_similar = pd.DataFrame(similar_data)
                df_similar.to_excel(writer, sheet_name='ç›¸ä¼¼é—œéµå­—çµæœ', index=False)
            else:
                # å¦‚æœæ²’æœ‰ç›¸ä¼¼é—œéµå­—ï¼Œå‰µå»ºç©ºå·¥ä½œè¡¨
                pd.DataFrame({'èªªæ˜': ['æœªæ‰¾åˆ°ç›¸ä¼¼åº¦è¼ƒé«˜çš„é—œéµå­—çµ„åˆ']}).to_excel(
                    writer, sheet_name='ç›¸ä¼¼é—œéµå­—çµæœ', index=False)
            
            # å·¥ä½œè¡¨3: å„æŒ‡æ¨™çµ±è¨ˆ
            print("  ğŸ“ˆ ç”Ÿæˆå·¥ä½œè¡¨3: å„æŒ‡æ¨™çµ±è¨ˆ")
            df_stats = self.generate_indicator_statistics(results)
            df_stats.to_excel(writer, sheet_name='å„æŒ‡æ¨™çµ±è¨ˆ', index=False)
            
            # å·¥ä½œè¡¨4: æ‘˜è¦çµ±è¨ˆ
            print("  ğŸ“Š ç”Ÿæˆå·¥ä½œè¡¨4: æ‘˜è¦çµ±è¨ˆ")
            summary = self.generate_summary_report(results)
            api_stats = self.api_manager.get_usage_statistics()
            
            summary_data = [
                {'é …ç›®': 'ç¸½é—œéµå­—æ•¸é‡', 'æ•¸å€¼': summary['total_keywords']},
                {'é …ç›®': 'æˆåŠŸæå–æ•¸é‡', 'æ•¸å€¼': summary['found_keywords']},
                {'é …ç›®': 'æœªæ‰¾åˆ°æ•¸é‡', 'æ•¸å€¼': summary['not_found_keywords']},
                {'é …ç›®': 'æ•´é«”æˆåŠŸç‡(%)', 'æ•¸å€¼': round(summary['found_keywords']/summary['total_keywords']*100, 1)},
                {'é …ç›®': 'é«˜ä¿¡å¿ƒçµæœæ•¸', 'æ•¸å€¼': summary['high_confidence_results']},
                {'é …ç›®': 'ç›¸ä¼¼é—œéµå­—çµ„æ•¸', 'æ•¸å€¼': len(similar_groups)},
                {'é …ç›®': 'ç¸½APIè«‹æ±‚æ¬¡æ•¸', 'æ•¸å€¼': api_stats['total_requests']},
                {'é …ç›®': 'ä½¿ç”¨çš„API Keyæ•¸é‡', 'æ•¸å€¼': len(GEMINI_API_KEYS)},
                {'é …ç›®': 'ç”Ÿæˆæ™‚é–“', 'æ•¸å€¼': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            ]
            
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='æ‘˜è¦çµ±è¨ˆ', index=False)
            
            # å·¥ä½œè¡¨5: APIä½¿ç”¨çµ±è¨ˆ
            print("  ğŸ”‘ ç”Ÿæˆå·¥ä½œè¡¨5: APIä½¿ç”¨çµ±è¨ˆ")
            api_data = []
            for key_name, key_stats in api_stats["keys_usage"].items():
                api_data.append({
                    'API Key': key_name,
                    'Keyé è¦½': key_stats['key_preview'],
                    'ä½¿ç”¨æ¬¡æ•¸': key_stats['usage_count'],
                    'ä½¿ç”¨æ¯”ä¾‹(%)': round(key_stats['usage_percentage'], 1),
                    'ç‹€æ…‹': 'å†·å»ä¸­' if key_stats['is_cooling'] else 'å¯ç”¨'
                })
            
            df_api = pd.DataFrame(api_data)
            df_api.to_excel(writer, sheet_name='APIä½¿ç”¨çµ±è¨ˆ', index=False)
        
        print(f"âœ… Excelå ±å‘Šç”Ÿæˆå®Œæˆ: {output_path}")
        return output_path

    def generate_summary_report(self, results: Dict[str, List[ExtractionResult]]) -> Dict:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        summary = {
            "total_keywords": 0,
            "found_keywords": 0,
            "not_found_keywords": 0,
            "high_confidence_results": 0,
            "indicators_summary": {}
        }
        
        for indicator, indicator_results in results.items():
            indicator_summary = {
                "total": len(indicator_results),
                "found": 0,
                "not_found": 0,
                "high_confidence": 0,
                "key_findings": []
            }
            
            for result in indicator_results:
                summary["total_keywords"] += 1
                
                if result.value in ["æœªæåŠ", "å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°"]:
                    summary["not_found_keywords"] += 1
                    indicator_summary["not_found"] += 1
                else:
                    summary["found_keywords"] += 1
                    indicator_summary["found"] += 1
                    
                    if result.confidence > 0.7:
                        summary["high_confidence_results"] += 1
                        indicator_summary["high_confidence"] += 1
                    
                    # æ”¶é›†é‡è¦ç™¼ç¾
                    if result.confidence > 0.6 and result.value not in ["æœªæåŠ", "å ±å‘Šæ›¸ä¸­æ²’æœ‰æåˆ°"]:
                        indicator_summary["key_findings"].append({
                            "keyword": result.keyword,
                            "value": result.value,
                            "confidence": result.confidence
                        })
            
            summary["indicators_summary"][indicator] = indicator_summary
        
        return summary

    def print_summary(self, summary: Dict, similar_groups: List[SimilarKeywordGroup]):
        """æ‰“å°æ‘˜è¦çµæœå’ŒAPIä½¿ç”¨çµ±è¨ˆ"""
        print("\n" + "="*60)
        print("ğŸ“Š ESGæ•¸æ“šæå–çµæœæ‘˜è¦ (å¤šAPI Keyç‰ˆæœ¬)")
        print("="*60)
        
        print(f"ç¸½é—œéµå­—æ•¸é‡: {summary['total_keywords']}")
        print(f"æˆåŠŸæå–: {summary['found_keywords']} ({summary['found_keywords']/summary['total_keywords']*100:.1f}%)")
        print(f"æœªæ‰¾åˆ°æ•¸æ“š: {summary['not_found_keywords']} ({summary['not_found_keywords']/summary['total_keywords']*100:.1f}%)")
        print(f"é«˜ä¿¡å¿ƒçµæœ: {summary['high_confidence_results']}")
        print(f"ç›¸ä¼¼é—œéµå­—çµ„: {len(similar_groups)}")
        
        # é¡¯ç¤ºAPIä½¿ç”¨çµ±è¨ˆ
        self.api_manager.print_usage_statistics()
        
        print("\nå„æŒ‡æ¨™è©³ç´°çµæœ:")
        print("-" * 60)
        
        for indicator, indicator_summary in summary["indicators_summary"].items():
            print(f"\nğŸ“ˆ {indicator}")
            print(f"   æˆåŠŸç‡: {indicator_summary['found']}/{indicator_summary['total']} "
                  f"({indicator_summary['found']/indicator_summary['total']*100:.1f}%)")
            
            # é¡¯ç¤ºé‡è¦ç™¼ç¾
            if indicator_summary['key_findings']:
                print("   ğŸ” é‡è¦ç™¼ç¾:")
                for finding in indicator_summary['key_findings'][:3]:
                    value_str = str(finding['value'])
                    if len(value_str) > 30:
                        value_str = value_str[:30] + "..."
                    print(f"      â€¢ {finding['keyword']}: {value_str} (ä¿¡å¿ƒåº¦:{finding['confidence']:.2f})")
        
        if similar_groups:
            print(f"\nğŸ”— ç›¸ä¼¼é—œéµå­—çµ„åˆ (å‰{min(5, len(similar_groups))}çµ„):")
            print("-" * 60)
            for i, group in enumerate(similar_groups[:5], 1):
                print(f"{i}. ç›¸ä¼¼åº¦: {group.similarity_score:.3f} | å…±åŒå€¼: {group.common_value}")
                print(f"   é—œéµå­—: {', '.join(group.keywords)}")
        
        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•¸ - ä½¿ç”¨å¤šAPI keyç”ŸæˆExcelå ±å‘Š"""
    
    try:
        print("ğŸš€ ESGæ•¸æ“šæå–å™¨ - å¤šAPI Keyç‰ˆæœ¬")
        print("=" * 60)
        
        # åˆå§‹åŒ–æå–å™¨
        extractor = MultiKeyESGDataExtractor()
        
        # æå–æ‰€æœ‰é—œéµå­—æ•¸æ“š
        results = extractor.extract_all_keywords()
        
        # ç”Ÿæˆæ‘˜è¦
        summary = extractor.generate_summary_report(results)
        
        # æ‰¾å‡ºç›¸ä¼¼é—œéµå­—
        similar_groups = extractor.find_similar_keywords(results)
        
        # ç”ŸæˆExcelå ±å‘Š
        excel_path = extractor.generate_excel_report(results)
        
        # æ‰“å°æ‘˜è¦
        extractor.print_summary(summary, similar_groups)
        
        print(f"\nğŸ‰ æå–å®Œæˆï¼")
        print(f"ğŸ“Š Excelå ±å‘Š: {excel_path}")
        print(f"ğŸ“‹ åŒ…å«å·¥ä½œè¡¨:")
        print(f"   â€¢ å·¥ä½œè¡¨1: å®Œæ•´æå–çµæœ")
        print(f"   â€¢ å·¥ä½œè¡¨2: ç›¸ä¼¼é—œéµå­—çµæœ")
        print(f"   â€¢ å·¥ä½œè¡¨3: å„æŒ‡æ¨™çµ±è¨ˆ")
        print(f"   â€¢ å·¥ä½œè¡¨4: æ‘˜è¦çµ±è¨ˆ")
        print(f"   â€¢ å·¥ä½œè¡¨5: APIä½¿ç”¨çµ±è¨ˆ")
        
        return results, summary, excel_path
        
    except Exception as e:
        print(f"âŒ æå–éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        return None, None, None

if __name__ == "__main__":
    main()